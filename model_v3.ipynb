{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72b4bd1d",
   "metadata": {},
   "source": [
    "# Model V3: Seed Averaging + Full Optuna Tuning\n",
    "\n",
    "**Improvements:**\n",
    "1. Optuna tuning for ALL models (LightGBM, XGBoost, CatBoost)\n",
    "2. Seed averaging (train with multiple seeds, average predictions)\n",
    "3. More Optuna trials for better hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "465621bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gallison/workspace/kaggle/diabetes_prediction_challenge/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder, TargetEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "import optuna\n",
    "from optuna_integration import LightGBMPruningCallback\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e44774d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (700000, 26), Test: (300000, 25)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "print(f\"Train: {train.shape}, Test: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bba25610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape after FE: (700000, 57), Test shape: (300000, 56)\n"
     ]
    }
   ],
   "source": [
    "def advanced_feature_engineering(df):\n",
    "    \"\"\"Advanced feature engineering\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # BMI categories\n",
    "    df['bmi_category'] = pd.cut(df['bmi'], bins=[0, 18.5, 25, 30, 100], \n",
    "                                 labels=['underweight', 'normal', 'overweight', 'obese'])\n",
    "    \n",
    "    # Age groups\n",
    "    df['age_group'] = pd.cut(df['age'], bins=[0, 30, 45, 60, 100], \n",
    "                              labels=['young', 'middle', 'senior', 'elderly'])\n",
    "    \n",
    "    # Blood pressure categories\n",
    "    df['bp_category'] = pd.cut(df['systolic_bp'], bins=[0, 120, 130, 140, 200], \n",
    "                                labels=['normal', 'elevated', 'high_stage1', 'high_stage2'])\n",
    "    \n",
    "    # Cholesterol ratios\n",
    "    df['ldl_hdl_ratio'] = df['ldl_cholesterol'] / (df['hdl_cholesterol'] + 1)\n",
    "    df['total_hdl_ratio'] = df['cholesterol_total'] / (df['hdl_cholesterol'] + 1)\n",
    "    df['non_hdl_cholesterol'] = df['cholesterol_total'] - df['hdl_cholesterol']\n",
    "    \n",
    "    # Risk score\n",
    "    df['medical_risk_score'] = (df['family_history_diabetes'] + \n",
    "                                 df['hypertension_history'] + \n",
    "                                 df['cardiovascular_history'])\n",
    "    \n",
    "    # Activity and lifestyle\n",
    "    df['activity_score'] = df['physical_activity_minutes_per_week'] / (df['screen_time_hours_per_day'] + 1)\n",
    "    df['lifestyle_score'] = (df['diet_score'] + df['sleep_hours_per_day'] + \n",
    "                              df['physical_activity_minutes_per_week'] / 60 - \n",
    "                              df['screen_time_hours_per_day'] - df['alcohol_consumption_per_week'])\n",
    "    \n",
    "    # Blood pressure\n",
    "    df['pulse_pressure'] = df['systolic_bp'] - df['diastolic_bp']\n",
    "    df['mean_arterial_pressure'] = df['diastolic_bp'] + (df['pulse_pressure'] / 3)\n",
    "    \n",
    "    # Interactions\n",
    "    df['age_bmi_interaction'] = df['age'] * df['bmi']\n",
    "    df['physical_activity_per_day'] = df['physical_activity_minutes_per_week'] / 7\n",
    "    df['age_systolic_interaction'] = df['age'] * df['systolic_bp']\n",
    "    df['bmi_triglycerides'] = df['bmi'] * df['triglycerides']\n",
    "    df['age_cholesterol'] = df['age'] * df['cholesterol_total']\n",
    "    df['activity_bmi_ratio'] = df['physical_activity_minutes_per_week'] / (df['bmi'] + 1)\n",
    "    \n",
    "    # Family history interactions\n",
    "    df['family_age_risk'] = df['family_history_diabetes'] * df['age']\n",
    "    df['family_bmi_risk'] = df['family_history_diabetes'] * df['bmi']\n",
    "    df['family_activity_protection'] = df['family_history_diabetes'] * df['physical_activity_minutes_per_week']\n",
    "    \n",
    "    # Metabolic syndrome indicators\n",
    "    df['metabolic_risk'] = ((df['bmi'] > 30).astype(int) + \n",
    "                            (df['triglycerides'] > 150).astype(int) + \n",
    "                            (df['hdl_cholesterol'] < 40).astype(int) +\n",
    "                            (df['systolic_bp'] > 130).astype(int))\n",
    "    \n",
    "    # Triglyceride to HDL ratio\n",
    "    df['tg_hdl_ratio'] = df['triglycerides'] / (df['hdl_cholesterol'] + 1)\n",
    "    \n",
    "    # Cardiovascular risk score\n",
    "    df['cv_risk_score'] = (df['systolic_bp'] / 10 + df['ldl_cholesterol'] / 10 + \n",
    "                           df['age'] / 5 - df['hdl_cholesterol'] / 10)\n",
    "    \n",
    "    # Lifestyle quality\n",
    "    df['healthy_lifestyle'] = ((df['physical_activity_minutes_per_week'] > 150).astype(int) +\n",
    "                                (df['diet_score'] > 7).astype(int) +\n",
    "                                (df['sleep_hours_per_day'] >= 7).astype(int) +\n",
    "                                (df['alcohol_consumption_per_week'] < 7).astype(int) +\n",
    "                                (df['screen_time_hours_per_day'] < 4).astype(int))\n",
    "    \n",
    "    # Age risk\n",
    "    df['age_risk'] = np.where(df['age'] < 40, 0, np.where(df['age'] < 50, 1, np.where(df['age'] < 60, 2, 3)))\n",
    "    \n",
    "    # Non-linear\n",
    "    df['bmi_squared'] = df['bmi'] ** 2\n",
    "    df['log_triglycerides'] = np.log1p(df['triglycerides'])\n",
    "    df['log_physical_activity'] = np.log1p(df['physical_activity_minutes_per_week'])\n",
    "    df['obesity_indicator'] = df['bmi'] * df['waist_to_hip_ratio']\n",
    "    df['poor_sleep'] = ((df['sleep_hours_per_day'] < 6) | (df['sleep_hours_per_day'] > 9)).astype(int)\n",
    "    df['sedentary'] = ((df['physical_activity_minutes_per_week'] < 75) & \n",
    "                        (df['screen_time_hours_per_day'] > 6)).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "train_fe = advanced_feature_engineering(train)\n",
    "test_fe = advanced_feature_engineering(test)\n",
    "print(f\"Train shape after FE: {train_fe.shape}, Test shape: {test_fe.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63722fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 55\n"
     ]
    }
   ],
   "source": [
    "# Prepare features\n",
    "target = 'diagnosed_diabetes'\n",
    "drop_cols = ['id', target, 'bmi_category', 'age_group', 'bp_category']\n",
    "cat_cols = ['bmi_category', 'age_group', 'bp_category']\n",
    "original_cat_cols = ['gender', 'ethnicity', 'education_level', 'income_level', 'smoking_status', 'employment_status']\n",
    "\n",
    "X = train_fe.drop(columns=[c for c in drop_cols if c in train_fe.columns])\n",
    "y = train_fe[target]\n",
    "X_test = test_fe.drop(columns=[c for c in drop_cols if c in test_fe.columns])\n",
    "\n",
    "# Label encode categorical columns\n",
    "for col in original_cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    all_values = pd.concat([X[col], X_test[col]], axis=0).unique()\n",
    "    le.fit(all_values)\n",
    "    X[col] = le.transform(X[col])\n",
    "    X_test[col] = le.transform(X_test[col])\n",
    "\n",
    "# Target encoding\n",
    "existing_cat_cols = [c for c in cat_cols if c in train_fe.columns]\n",
    "target_encoder = TargetEncoder(target_type='binary', smooth='auto')\n",
    "train_cats = train_fe[existing_cat_cols].copy()\n",
    "test_cats = test_fe[existing_cat_cols].copy()\n",
    "target_encoder.fit(train_cats, y)\n",
    "train_encoded = target_encoder.transform(train_cats)\n",
    "test_encoded = target_encoder.transform(test_cats)\n",
    "\n",
    "for i, col in enumerate(existing_cat_cols):\n",
    "    X[f'{col}_target_enc'] = train_encoded[:, i]\n",
    "    X_test[f'{col}_target_enc'] = test_encoded[:, i]\n",
    "\n",
    "print(f\"Features: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ae601c",
   "metadata": {},
   "source": [
    "## Optuna Tuning for All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c32306e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning LightGBM (50 trials)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 16. Best value: 0.725883: 100%|██████████| 50/50 [36:18<00:00, 43.58s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best LightGBM AUC: 0.72588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# LightGBM Optuna objective\n",
    "def objective_lgb(trial):\n",
    "    params = {\n",
    "        'objective': 'binary', 'metric': 'auc', 'boosting_type': 'gbdt',\n",
    "        'verbosity': -1, 'n_jobs': -1, 'random_state': 42,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "    }\n",
    "    \n",
    "    cv_scores = []\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n",
    "                  callbacks=[lgb.early_stopping(50, verbose=False), LightGBMPruningCallback(trial, 'auc')])\n",
    "        cv_scores.append(roc_auc_score(y_val, model.predict_proba(X_val)[:, 1]))\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "print(\"Tuning LightGBM (50 trials)...\")\n",
    "study_lgb = optuna.create_study(direction='maximize')\n",
    "study_lgb.optimize(objective_lgb, n_trials=50, show_progress_bar=True)\n",
    "print(f\"Best LightGBM AUC: {study_lgb.best_value:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "888009f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning XGBoost (50 trials)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 11. Best value: 0.725713: 100%|██████████| 50/50 [2:30:23<00:00, 180.46s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost AUC: 0.72571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# XGBoost Optuna objective\n",
    "def objective_xgb(trial):\n",
    "    params = {\n",
    "        'objective': 'binary:logistic', 'eval_metric': 'auc',\n",
    "        'verbosity': 0, 'n_jobs': -1, 'random_state': 42,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "    }\n",
    "    \n",
    "    cv_scores = []\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "        cv_scores.append(roc_auc_score(y_val, model.predict_proba(X_val)[:, 1]))\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "print(\"Tuning XGBoost (50 trials)...\")\n",
    "study_xgb = optuna.create_study(direction='maximize')\n",
    "study_xgb.optimize(objective_xgb, n_trials=50, show_progress_bar=True)\n",
    "print(f\"Best XGBoost AUC: {study_xgb.best_value:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bdba1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning CatBoost (30 trials - slower)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 27. Best value: 0.725749: 100%|██████████| 30/30 [1:17:14<00:00, 154.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CatBoost AUC: 0.72575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# CatBoost Optuna objective\n",
    "def objective_cat(trial):\n",
    "    params = {\n",
    "        'eval_metric': 'AUC', 'verbose': False, 'random_seed': 42,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 1),\n",
    "    }\n",
    "    \n",
    "    cv_scores = []\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        model = CatBoostClassifier(**params)\n",
    "        model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50, verbose=False)\n",
    "        cv_scores.append(roc_auc_score(y_val, model.predict_proba(X_val)[:, 1]))\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "print(\"Tuning CatBoost (30 trials - slower)...\")\n",
    "study_cat = optuna.create_study(direction='maximize')\n",
    "study_cat.optimize(objective_cat, n_trials=30, show_progress_bar=True)\n",
    "print(f\"Best CatBoost AUC: {study_cat.best_value:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bceacf",
   "metadata": {},
   "source": [
    "## Seed Averaging Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4534ea25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params configured:\n",
      "LightGBM: 0.72588\n",
      "XGBoost: 0.72571\n",
      "CatBoost: 0.72575\n"
     ]
    }
   ],
   "source": [
    "# Prepare best params from Optuna\n",
    "best_lgb_params = study_lgb.best_params.copy()\n",
    "best_lgb_params.update({'objective': 'binary', 'metric': 'auc', 'boosting_type': 'gbdt', 'verbosity': -1, 'n_jobs': -1})\n",
    "\n",
    "best_xgb_params = study_xgb.best_params.copy()\n",
    "best_xgb_params.update({'objective': 'binary:logistic', 'eval_metric': 'auc', 'verbosity': 0, 'n_jobs': -1})\n",
    "\n",
    "best_cat_params = study_cat.best_params.copy()\n",
    "best_cat_params.update({'eval_metric': 'AUC', 'verbose': False})\n",
    "\n",
    "print(\"Best params configured:\")\n",
    "print(f\"LightGBM: {study_lgb.best_value:.5f}\")\n",
    "print(f\"XGBoost: {study_xgb.best_value:.5f}\")\n",
    "print(f\"CatBoost: {study_cat.best_value:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3e772e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with seed averaging (5 seeds x 5 folds)...\n",
      "\n",
      "=== Seed 42 (1/5) ===\n",
      "  LGB: 0.72588, XGB: 0.72571, CAT: 0.72574\n",
      "=== Seed 123 (2/5) ===\n",
      "  LGB: 0.72568, XGB: 0.72605, CAT: 0.72541\n",
      "=== Seed 456 (3/5) ===\n",
      "  LGB: 0.72566, XGB: 0.72594, CAT: 0.72579\n",
      "=== Seed 789 (4/5) ===\n",
      "  LGB: 0.72565, XGB: 0.72596, CAT: 0.72555\n",
      "=== Seed 2024 (5/5) ===\n",
      "  LGB: 0.72581, XGB: 0.72561, CAT: 0.72581\n",
      "\n",
      "==================================================\n",
      "Seed-Averaged CV AUC:\n",
      "  LightGBM: 0.72731\n",
      "  XGBoost:  0.72750\n",
      "  CatBoost: 0.72666\n"
     ]
    }
   ],
   "source": [
    "# Seed averaging - train with multiple seeds and average predictions\n",
    "SEEDS = [42, 123, 456, 789, 2024]\n",
    "n_splits = 5\n",
    "\n",
    "# Storage for predictions\n",
    "all_oof_lgb = np.zeros((len(SEEDS), len(X)))\n",
    "all_oof_xgb = np.zeros((len(SEEDS), len(X)))\n",
    "all_oof_cat = np.zeros((len(SEEDS), len(X)))\n",
    "\n",
    "all_test_lgb = np.zeros((len(SEEDS), len(X_test)))\n",
    "all_test_xgb = np.zeros((len(SEEDS), len(X_test)))\n",
    "all_test_cat = np.zeros((len(SEEDS), len(X_test)))\n",
    "\n",
    "print(\"Training with seed averaging (5 seeds x 5 folds)...\\n\")\n",
    "\n",
    "for seed_idx, seed in enumerate(SEEDS):\n",
    "    print(f\"=== Seed {seed} ({seed_idx+1}/{len(SEEDS)}) ===\")\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    \n",
    "    oof_lgb = np.zeros(len(X))\n",
    "    oof_xgb = np.zeros(len(X))\n",
    "    oof_cat = np.zeros(len(X))\n",
    "    test_lgb = np.zeros(len(X_test))\n",
    "    test_xgb = np.zeros(len(X_test))\n",
    "    test_cat = np.zeros(len(X_test))\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # LightGBM\n",
    "        lgb_params = best_lgb_params.copy()\n",
    "        lgb_params['random_state'] = seed\n",
    "        lgb_model = lgb.LGBMClassifier(**lgb_params)\n",
    "        lgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "        oof_lgb[val_idx] = lgb_model.predict_proba(X_val)[:, 1]\n",
    "        test_lgb += lgb_model.predict_proba(X_test)[:, 1] / n_splits\n",
    "        \n",
    "        # XGBoost\n",
    "        xgb_params = best_xgb_params.copy()\n",
    "        xgb_params['random_state'] = seed\n",
    "        xgb_model = xgb.XGBClassifier(**xgb_params)\n",
    "        xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "        oof_xgb[val_idx] = xgb_model.predict_proba(X_val)[:, 1]\n",
    "        test_xgb += xgb_model.predict_proba(X_test)[:, 1] / n_splits\n",
    "        \n",
    "        # CatBoost\n",
    "        cat_params = best_cat_params.copy()\n",
    "        cat_params['random_seed'] = seed\n",
    "        cat_model = CatBoostClassifier(**cat_params)\n",
    "        cat_model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50, verbose=False)\n",
    "        oof_cat[val_idx] = cat_model.predict_proba(X_val)[:, 1]\n",
    "        test_cat += cat_model.predict_proba(X_test)[:, 1] / n_splits\n",
    "    \n",
    "    all_oof_lgb[seed_idx] = oof_lgb\n",
    "    all_oof_xgb[seed_idx] = oof_xgb\n",
    "    all_oof_cat[seed_idx] = oof_cat\n",
    "    all_test_lgb[seed_idx] = test_lgb\n",
    "    all_test_xgb[seed_idx] = test_xgb\n",
    "    all_test_cat[seed_idx] = test_cat\n",
    "    \n",
    "    print(f\"  LGB: {roc_auc_score(y, oof_lgb):.5f}, XGB: {roc_auc_score(y, oof_xgb):.5f}, CAT: {roc_auc_score(y, oof_cat):.5f}\")\n",
    "\n",
    "# Average across seeds\n",
    "oof_lgb_avg = all_oof_lgb.mean(axis=0)\n",
    "oof_xgb_avg = all_oof_xgb.mean(axis=0)\n",
    "oof_cat_avg = all_oof_cat.mean(axis=0)\n",
    "test_lgb_avg = all_test_lgb.mean(axis=0)\n",
    "test_xgb_avg = all_test_xgb.mean(axis=0)\n",
    "test_cat_avg = all_test_cat.mean(axis=0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Seed-Averaged CV AUC:\")\n",
    "print(f\"  LightGBM: {roc_auc_score(y, oof_lgb_avg):.5f}\")\n",
    "print(f\"  XGBoost:  {roc_auc_score(y, oof_xgb_avg):.5f}\")\n",
    "print(f\"  CatBoost: {roc_auc_score(y, oof_cat_avg):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f70d82",
   "metadata": {},
   "source": [
    "## Optimize Ensemble Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e56f8bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing ensemble weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 189. Best value: 0.728072: 100%|██████████| 200/200 [00:28<00:00,  6.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal weights: LGB=0.468, XGB=0.532, CAT=0.000\n",
      "Weighted Ensemble CV AUC: 0.72807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Optimize ensemble weights\n",
    "def objective_weights(trial):\n",
    "    w_lgb = trial.suggest_float('w_lgb', 0, 1)\n",
    "    w_xgb = trial.suggest_float('w_xgb', 0, 1)\n",
    "    w_cat = trial.suggest_float('w_cat', 0, 1)\n",
    "    total = w_lgb + w_xgb + w_cat\n",
    "    w_lgb, w_xgb, w_cat = w_lgb/total, w_xgb/total, w_cat/total\n",
    "    ensemble_oof = w_lgb * oof_lgb_avg + w_xgb * oof_xgb_avg + w_cat * oof_cat_avg\n",
    "    return roc_auc_score(y, ensemble_oof)\n",
    "\n",
    "print(\"Optimizing ensemble weights...\")\n",
    "study_weights = optuna.create_study(direction='maximize')\n",
    "study_weights.optimize(objective_weights, n_trials=200, show_progress_bar=True)\n",
    "\n",
    "# Get optimal weights\n",
    "bw = study_weights.best_params\n",
    "total = bw['w_lgb'] + bw['w_xgb'] + bw['w_cat']\n",
    "w_lgb, w_xgb, w_cat = bw['w_lgb']/total, bw['w_xgb']/total, bw['w_cat']/total\n",
    "\n",
    "print(f\"\\nOptimal weights: LGB={w_lgb:.3f}, XGB={w_xgb:.3f}, CAT={w_cat:.3f}\")\n",
    "print(f\"Weighted Ensemble CV AUC: {study_weights.best_value:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012bc051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to submission_v3.csv\n",
      "Shape: (300000, 2)\n",
      "\n",
      "Prediction stats:\n",
      "count    300000.000000\n",
      "mean          0.601070\n",
      "std           0.194198\n",
      "min           0.042612\n",
      "25%           0.460732\n",
      "50%           0.606089\n",
      "75%           0.749255\n",
      "max           0.994643\n",
      "Name: diagnosed_diabetes, dtype: float64\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Create final predictions\n",
    "final_preds = w_lgb * test_lgb_avg + w_xgb * test_xgb_avg + w_cat * test_cat_avg\n",
    "\n",
    "# Create submission\n",
    "submission = pd.DataFrame({'id': test['id'], 'diagnosed_diabetes': final_preds})\n",
    "submission.to_csv('submission_v3.csv', index=False)\n",
    "\n",
    "print(f\"Submission saved to submission_v3.csv\")\n",
    "print(f\"Shape: {submission.shape}\")\n",
    "print(f\"\\nPrediction stats:\\n{submission['diagnosed_diabetes'].describe()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
