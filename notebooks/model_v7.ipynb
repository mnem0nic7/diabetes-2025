{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43ef59cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from scipy.stats import rankdata\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42\n",
    "N_SPLITS = 10\n",
    "TARGET = 'diagnosed_diabetes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9f8bca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (700000, 26)\n",
      "Test shape: (300000, 25)\n",
      "Target rate: 0.6233\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"Target rate: {train[TARGET].mean():.4f}\")\n",
    "\n",
    "test_ids = test['id']\n",
    "train = train.drop(columns=['id'])\n",
    "test = test.drop(columns=['id'])\n",
    "\n",
    "def prepare_data(df):\n",
    "    df = df.copy()\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].astype('category')\n",
    "    return df\n",
    "\n",
    "train = prepare_data(train)\n",
    "test = prepare_data(test)\n",
    "\n",
    "y = train[TARGET]\n",
    "X = train.drop(columns=[TARGET])\n",
    "X_test = test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79e9c40",
   "metadata": {},
   "source": [
    "## Part 1: Path Smoothing Regularization\n",
    "\n",
    "Path smoothing prevents overfitting by smoothing leaf weights toward parent node weights.\n",
    "This is especially useful when leaves have few samples (common with distribution shift)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd44308e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Path Smooth Regularization ===\n",
      "path_smooth=5 -> CV AUC 0.72853 | saved submission_v7_pathsmooth_5.csv\n",
      "path_smooth=10 -> CV AUC 0.72853 | saved submission_v7_pathsmooth_10.csv\n",
      "path_smooth=20 -> CV AUC 0.72853 | saved submission_v7_pathsmooth_20.csv\n"
     ]
    }
   ],
   "source": [
    "def train_lgb_path_smooth(X, y, X_test, n_splits=10, path_smooth=10.0, seed=42):\n",
    "    \"\"\"LightGBM with path_smooth regularization\"\"\"\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'n_estimators': 5000,\n",
    "        'learning_rate': 0.01,\n",
    "        'num_leaves': 31,\n",
    "        'max_depth': 6,\n",
    "        'min_child_samples': 50,\n",
    "        'feature_fraction': 0.7,\n",
    "        'bagging_fraction': 0.7,\n",
    "        'bagging_freq': 5,\n",
    "        'reg_alpha': 0.5,\n",
    "        'reg_lambda': 0.5,\n",
    "        'path_smooth': path_smooth,  # KEY: smooths leaf weights toward parent\n",
    "        'random_state': seed,\n",
    "        'verbose': -1,\n",
    "    }\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    oof_preds = np.zeros(len(X))\n",
    "    test_preds = np.zeros(len(X_test))\n",
    "    \n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(X, y)):\n",
    "        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "        \n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_va, y_va)],\n",
    "            callbacks=[lgb.early_stopping(100, verbose=False)]\n",
    "        )\n",
    "        \n",
    "        oof_preds[va_idx] = model.predict_proba(X_va)[:, 1]\n",
    "        test_preds += model.predict_proba(X_test)[:, 1] / n_splits\n",
    "    \n",
    "    oof_auc = roc_auc_score(y, oof_preds)\n",
    "    return oof_preds, test_preds, oof_auc\n",
    "\n",
    "# Try different path_smooth values\n",
    "print(\"=== Path Smooth Regularization ===\")\n",
    "for ps in [5.0, 10.0, 20.0]:\n",
    "    oof, preds, auc = train_lgb_path_smooth(X, y, X_test, n_splits=N_SPLITS, path_smooth=ps, seed=SEED)\n",
    "    out = f'submission_v7_pathsmooth_{int(ps)}.csv'\n",
    "    pd.DataFrame({'id': test_ids, 'diagnosed_diabetes': preds}).to_csv(out, index=False)\n",
    "    print(f\"path_smooth={ps:.0f} -> CV AUC {auc:.5f} | saved {out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339080bd",
   "metadata": {},
   "source": [
    "## Part 2: Multi-Seed Averaging\n",
    "\n",
    "Train the same model with different random seeds and average predictions.\n",
    "This reduces variance and often improves generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d225ee67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Multi-Seed Averaging (5 seeds) ===\n",
      "Training with seeds: [42, 1042, 2042, 3042, 4042]\n",
      "  Seed 42: CV AUC = 0.72853\n",
      "  Seed 1042: CV AUC = 0.72836\n",
      "  Seed 2042: CV AUC = 0.72839\n",
      "  Seed 3042: CV AUC = 0.72844\n",
      "  Seed 4042: CV AUC = 0.72838\n",
      "Averaged CV AUC: 0.72871\n",
      "Saved: submission_v7_multiseed_5.csv\n"
     ]
    }
   ],
   "source": [
    "def train_lgb_multiseed(X, y, X_test, n_splits=10, n_seeds=5, base_seed=42):\n",
    "    \"\"\"Train LightGBM with multiple seeds and average\"\"\"\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'n_estimators': 5000,\n",
    "        'learning_rate': 0.01,\n",
    "        'num_leaves': 31,\n",
    "        'max_depth': 6,\n",
    "        'min_child_samples': 50,\n",
    "        'feature_fraction': 0.7,\n",
    "        'bagging_fraction': 0.7,\n",
    "        'bagging_freq': 5,\n",
    "        'reg_alpha': 0.5,\n",
    "        'reg_lambda': 0.5,\n",
    "        'verbose': -1,\n",
    "    }\n",
    "    \n",
    "    all_oof = []\n",
    "    all_test = []\n",
    "    \n",
    "    seeds = [base_seed + i * 1000 for i in range(n_seeds)]\n",
    "    print(f\"Training with seeds: {seeds}\")\n",
    "    \n",
    "    for seed in seeds:\n",
    "        params['random_state'] = seed\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "        oof_preds = np.zeros(len(X))\n",
    "        test_preds = np.zeros(len(X_test))\n",
    "        \n",
    "        for fold, (tr_idx, va_idx) in enumerate(skf.split(X, y)):\n",
    "            X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "            y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "            \n",
    "            model = lgb.LGBMClassifier(**params)\n",
    "            model.fit(\n",
    "                X_tr, y_tr,\n",
    "                eval_set=[(X_va, y_va)],\n",
    "                callbacks=[lgb.early_stopping(100, verbose=False)]\n",
    "            )\n",
    "            \n",
    "            oof_preds[va_idx] = model.predict_proba(X_va)[:, 1]\n",
    "            test_preds += model.predict_proba(X_test)[:, 1] / n_splits\n",
    "        \n",
    "        seed_auc = roc_auc_score(y, oof_preds)\n",
    "        print(f\"  Seed {seed}: CV AUC = {seed_auc:.5f}\")\n",
    "        all_oof.append(oof_preds)\n",
    "        all_test.append(test_preds)\n",
    "    \n",
    "    # Average across seeds\n",
    "    avg_oof = np.mean(all_oof, axis=0)\n",
    "    avg_test = np.mean(all_test, axis=0)\n",
    "    avg_auc = roc_auc_score(y, avg_oof)\n",
    "    \n",
    "    return avg_oof, avg_test, avg_auc, all_oof, all_test\n",
    "\n",
    "print(\"\\n=== Multi-Seed Averaging (5 seeds) ===\")\n",
    "oof_ms, test_ms, auc_ms, all_oof, all_test = train_lgb_multiseed(X, y, X_test, n_splits=N_SPLITS, n_seeds=5)\n",
    "print(f\"Averaged CV AUC: {auc_ms:.5f}\")\n",
    "\n",
    "pd.DataFrame({'id': test_ids, 'diagnosed_diabetes': test_ms}).to_csv('submission_v7_multiseed_5.csv', index=False)\n",
    "print(\"Saved: submission_v7_multiseed_5.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32019bc1",
   "metadata": {},
   "source": [
    "## Part 3: Stacked Generalization (Level-2 Meta-Model)\n",
    "\n",
    "Use OOF predictions from diverse base models as features for a meta-learner.\n",
    "This can capture complementary patterns from different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0538e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Base Models for Stacking ===\n",
      "Training LightGBM...\n",
      "  LGB CV: 0.72707\n",
      "Training CatBoost...\n",
      "  CAT CV: 0.72287\n",
      "Training XGBoost...\n",
      "  XGB CV: 0.72627\n"
     ]
    }
   ],
   "source": [
    "def train_base_models(X, y, X_test, n_splits=10, seed=42):\n",
    "    \"\"\"Train diverse base models and return OOF predictions\"\"\"\n",
    "    \n",
    "    base_oof = {}\n",
    "    base_test = {}\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    \n",
    "    # Model 1: LightGBM (default)\n",
    "    print(\"Training LightGBM...\")\n",
    "    lgb_oof = np.zeros(len(X))\n",
    "    lgb_test = np.zeros(len(X_test))\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(X, y)):\n",
    "        model = lgb.LGBMClassifier(\n",
    "            n_estimators=3000, learning_rate=0.01, num_leaves=31, max_depth=6,\n",
    "            min_child_samples=50, feature_fraction=0.7, bagging_fraction=0.7,\n",
    "            bagging_freq=5, reg_alpha=0.5, reg_lambda=0.5, random_state=seed, verbose=-1\n",
    "        )\n",
    "        model.fit(X.iloc[tr_idx], y.iloc[tr_idx], eval_set=[(X.iloc[va_idx], y.iloc[va_idx])],\n",
    "                  callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "        lgb_oof[va_idx] = model.predict_proba(X.iloc[va_idx])[:, 1]\n",
    "        lgb_test += model.predict_proba(X_test)[:, 1] / n_splits\n",
    "    base_oof['lgb'] = lgb_oof\n",
    "    base_test['lgb'] = lgb_test\n",
    "    print(f\"  LGB CV: {roc_auc_score(y, lgb_oof):.5f}\")\n",
    "    \n",
    "    # Model 2: CatBoost\n",
    "    print(\"Training CatBoost...\")\n",
    "    cat_cols = X.select_dtypes(include=['category']).columns.tolist()\n",
    "    cat_oof = np.zeros(len(X))\n",
    "    cat_test = np.zeros(len(X_test))\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(X, y)):\n",
    "        model = cb.CatBoostClassifier(\n",
    "            iterations=2000, learning_rate=0.02, depth=5, l2_leaf_reg=5,\n",
    "            random_seed=seed+fold, verbose=False, early_stopping_rounds=150\n",
    "        )\n",
    "        model.fit(X.iloc[tr_idx], y.iloc[tr_idx], cat_features=cat_cols,\n",
    "                  eval_set=(X.iloc[va_idx], y.iloc[va_idx]))\n",
    "        cat_oof[va_idx] = model.predict_proba(X.iloc[va_idx])[:, 1]\n",
    "        cat_test += model.predict_proba(X_test)[:, 1] / n_splits\n",
    "    base_oof['cat'] = cat_oof\n",
    "    base_test['cat'] = cat_test\n",
    "    print(f\"  CAT CV: {roc_auc_score(y, cat_oof):.5f}\")\n",
    "    \n",
    "    # Model 3: XGBoost\n",
    "    print(\"Training XGBoost...\")\n",
    "    X_enc = X.copy()\n",
    "    X_test_enc = X_test.copy()\n",
    "    for col in X_enc.select_dtypes(include=['category']).columns:\n",
    "        X_enc[col] = X_enc[col].cat.codes\n",
    "        X_test_enc[col] = X_test_enc[col].cat.codes\n",
    "    \n",
    "    xgb_oof = np.zeros(len(X))\n",
    "    xgb_test = np.zeros(len(X_test))\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(X_enc, y)):\n",
    "        model = xgb.XGBClassifier(\n",
    "            n_estimators=2000, learning_rate=0.02, max_depth=5, subsample=0.7,\n",
    "            colsample_bytree=0.5, reg_lambda=2.0, reg_alpha=0.5,\n",
    "            random_state=seed+fold, tree_method='hist', early_stopping_rounds=150\n",
    "        )\n",
    "        model.fit(X_enc.iloc[tr_idx], y.iloc[tr_idx],\n",
    "                  eval_set=[(X_enc.iloc[va_idx], y.iloc[va_idx])], verbose=False)\n",
    "        xgb_oof[va_idx] = model.predict_proba(X_enc.iloc[va_idx])[:, 1]\n",
    "        xgb_test += model.predict_proba(X_test_enc)[:, 1] / n_splits\n",
    "    base_oof['xgb'] = xgb_oof\n",
    "    base_test['xgb'] = xgb_test\n",
    "    print(f\"  XGB CV: {roc_auc_score(y, xgb_oof):.5f}\")\n",
    "    \n",
    "    return base_oof, base_test\n",
    "\n",
    "print(\"\\n=== Training Base Models for Stacking ===\")\n",
    "base_oof, base_test = train_base_models(X, y, X_test, n_splits=N_SPLITS, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d47afe82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Meta-Model Training ===\n",
      "Simple average: CV AUC = 0.72599\n",
      "Rank average: CV AUC = 0.72595\n",
      "LR meta-model: CV AUC = 0.72778\n",
      "\n",
      "Saved: submission_v7_stack_avg.csv, submission_v7_stack_rank.csv, submission_v7_stack_lr.csv\n"
     ]
    }
   ],
   "source": [
    "# Create stacking features\n",
    "stack_train = pd.DataFrame(base_oof)\n",
    "stack_test = pd.DataFrame(base_test)\n",
    "\n",
    "print(\"\\n=== Meta-Model Training ===\")\n",
    "\n",
    "# Method 1: Simple averaging (baseline)\n",
    "avg_oof = stack_train.mean(axis=1)\n",
    "avg_test = stack_test.mean(axis=1)\n",
    "print(f\"Simple average: CV AUC = {roc_auc_score(y, avg_oof):.5f}\")\n",
    "\n",
    "# Method 2: Rank averaging\n",
    "rank_oof = stack_train.apply(lambda x: rankdata(x) / len(x)).mean(axis=1)\n",
    "rank_test = stack_test.apply(lambda x: rankdata(x) / len(x)).mean(axis=1)\n",
    "print(f\"Rank average: CV AUC = {roc_auc_score(y, rank_oof):.5f}\")\n",
    "\n",
    "# Method 3: Logistic Regression meta-model\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "lr_oof = np.zeros(len(y))\n",
    "lr_test = np.zeros(len(X_test))\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf.split(stack_train, y)):\n",
    "    meta = LogisticRegression(C=1.0, max_iter=1000, random_state=SEED)\n",
    "    meta.fit(stack_train.iloc[tr_idx], y.iloc[tr_idx])\n",
    "    lr_oof[va_idx] = meta.predict_proba(stack_train.iloc[va_idx])[:, 1]\n",
    "    lr_test += meta.predict_proba(stack_test)[:, 1] / 5\n",
    "\n",
    "print(f\"LR meta-model: CV AUC = {roc_auc_score(y, lr_oof):.5f}\")\n",
    "\n",
    "# Save stacking submissions\n",
    "pd.DataFrame({'id': test_ids, 'diagnosed_diabetes': avg_test}).to_csv('submission_v7_stack_avg.csv', index=False)\n",
    "pd.DataFrame({'id': test_ids, 'diagnosed_diabetes': rank_test}).to_csv('submission_v7_stack_rank.csv', index=False)\n",
    "pd.DataFrame({'id': test_ids, 'diagnosed_diabetes': lr_test}).to_csv('submission_v7_stack_lr.csv', index=False)\n",
    "print(\"\\nSaved: submission_v7_stack_avg.csv, submission_v7_stack_rank.csv, submission_v7_stack_lr.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04e73a2",
   "metadata": {},
   "source": [
    "## Part 4: Target Distribution Calibration\n",
    "\n",
    "If the test target distribution differs from train, calibrating predictions can help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bddb8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train positive rate: 0.6233\n",
      "\n",
      "=== Target Distribution Calibration ===\n",
      "Calibrated to 60% -> saved submission_v7_calib_60.csv\n",
      "Calibrated to 62% -> saved submission_v7_calib_62.csv\n",
      "Calibrated to 65% -> saved submission_v7_calib_65.csv\n"
     ]
    }
   ],
   "source": [
    "def calibrate_to_prior(preds, target_prior):\n",
    "    \"\"\"Calibrate predictions to match a target prior (positive rate)\"\"\"\n",
    "    # Use Platt scaling idea: adjust threshold\n",
    "    current_mean = preds.mean()\n",
    "    if abs(current_mean - target_prior) < 0.001:\n",
    "        return preds\n",
    "    \n",
    "    # Simple linear scaling to match target prior\n",
    "    # Find scaling factor that shifts mean to target\n",
    "    scaled = preds.copy()\n",
    "    \n",
    "    # Use rank-based calibration (preserves ordering)\n",
    "    ranks = rankdata(preds) / len(preds)\n",
    "    \n",
    "    # Map to target distribution\n",
    "    from scipy.stats import norm\n",
    "    # Transform to normal, shift, transform back\n",
    "    z_scores = norm.ppf(np.clip(ranks, 0.001, 0.999))\n",
    "    # Shift mean to match target prior in probability space\n",
    "    target_z = norm.ppf(target_prior)\n",
    "    current_z = z_scores.mean()\n",
    "    shifted_z = z_scores - current_z + target_z\n",
    "    calibrated = norm.cdf(shifted_z)\n",
    "    \n",
    "    return calibrated\n",
    "\n",
    "# Train target rate\n",
    "train_prior = y.mean()\n",
    "print(f\"Train positive rate: {train_prior:.4f}\")\n",
    "\n",
    "# Try different target priors (in case test has different rate)\n",
    "print(\"\\n=== Target Distribution Calibration ===\")\n",
    "for target_prior in [0.60, 0.62, 0.65]:\n",
    "    calib_preds = calibrate_to_prior(test_ms, target_prior)\n",
    "    out = f'submission_v7_calib_{int(target_prior*100)}.csv'\n",
    "    pd.DataFrame({'id': test_ids, 'diagnosed_diabetes': calib_preds}).to_csv(out, index=False)\n",
    "    print(f\"Calibrated to {target_prior:.0%} -> saved {out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a930fd0",
   "metadata": {},
   "source": [
    "## Part 5: Blend Best V6 Submissions with V7\n",
    "\n",
    "Combine best previous submissions with new V7 predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3c2d842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded V5_LGB (LB: 0.69771)\n",
      "Loaded V1 (LB: 0.6972)\n",
      "Loaded V6_QN\n",
      "\n",
      "=== Final Blend ===\n",
      "Saved: submission_v7_final_blend.csv (V7*0.4 + V5_LGB*0.35 + V1*0.25)\n",
      "Saved: submission_v7_final_rank.csv (rank blend)\n"
     ]
    }
   ],
   "source": [
    "# Load best previous submissions\n",
    "prev_subs = {}\n",
    "prev_files = [\n",
    "    ('submission_v5_lgb_simple.csv', 'V5_LGB', 0.69771),\n",
    "    ('submission.csv', 'V1', 0.69720),\n",
    "    ('submission_v6_quantile_norm.csv', 'V6_QN', None),\n",
    "]\n",
    "\n",
    "for fname, name, lb in prev_files:\n",
    "    if os.path.exists(fname):\n",
    "        prev_subs[name] = pd.read_csv(fname)['diagnosed_diabetes'].values\n",
    "        print(f\"Loaded {name}\" + (f\" (LB: {lb})\" if lb else \"\"))\n",
    "\n",
    "# Create final blend\n",
    "print(\"\\n=== Final Blend ===\")\n",
    "if 'V5_LGB' in prev_subs and 'V1' in prev_subs:\n",
    "    # Blend: V7 multi-seed (new) + V5_LGB (best LB) + V1 (stable)\n",
    "    final_blend = (\n",
    "        test_ms * 0.4 +           # V7 multi-seed\n",
    "        prev_subs['V5_LGB'] * 0.35 +  # V5 LGB (best LB)\n",
    "        prev_subs['V1'] * 0.25       # V1 (stable)\n",
    "    )\n",
    "    pd.DataFrame({'id': test_ids, 'diagnosed_diabetes': final_blend}).to_csv('submission_v7_final_blend.csv', index=False)\n",
    "    print(\"Saved: submission_v7_final_blend.csv (V7*0.4 + V5_LGB*0.35 + V1*0.25)\")\n",
    "    \n",
    "    # Also try rank blend\n",
    "    r1 = rankdata(test_ms) / len(test_ms)\n",
    "    r2 = rankdata(prev_subs['V5_LGB']) / len(prev_subs['V5_LGB'])\n",
    "    r3 = rankdata(prev_subs['V1']) / len(prev_subs['V1'])\n",
    "    rank_final = (r1 * 0.4 + r2 * 0.35 + r3 * 0.25)\n",
    "    pd.DataFrame({'id': test_ids, 'diagnosed_diabetes': rank_final}).to_csv('submission_v7_final_rank.csv', index=False)\n",
    "    print(\"Saved: submission_v7_final_rank.csv (rank blend)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696c8d06",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2477cd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "V7 SUBMISSIONS READY FOR APPROVAL\n",
      "============================================================\n",
      "\n",
      "Submissions created:\n",
      "  1. submission_v7_pathsmooth_5.csv\n",
      "  2. submission_v7_pathsmooth_10.csv\n",
      "  3. submission_v7_pathsmooth_20.csv\n",
      "  4. submission_v7_multiseed_5.csv\n",
      "  5. submission_v7_stack_avg.csv\n",
      "  6. submission_v7_stack_rank.csv\n",
      "  7. submission_v7_stack_lr.csv\n",
      "  8. submission_v7_calib_60.csv\n",
      "  9. submission_v7_calib_62.csv\n",
      "  10. submission_v7_calib_65.csv\n",
      "  11. submission_v7_final_blend.csv\n",
      "  12. submission_v7_final_rank.csv\n",
      "\n",
      "============================================================\n",
      "TOP PICKS TO TRY:\n",
      "  1. submission_v7_multiseed_5.csv - reduces variance\n",
      "  2. submission_v7_stack_rank.csv - diverse model blend\n",
      "  3. submission_v7_final_blend.csv - combines best of all\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"V7 SUBMISSIONS READY FOR APPROVAL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "v7_files = [\n",
    "    'submission_v7_pathsmooth_5.csv',\n",
    "    'submission_v7_pathsmooth_10.csv', \n",
    "    'submission_v7_pathsmooth_20.csv',\n",
    "    'submission_v7_multiseed_5.csv',\n",
    "    'submission_v7_stack_avg.csv',\n",
    "    'submission_v7_stack_rank.csv',\n",
    "    'submission_v7_stack_lr.csv',\n",
    "    'submission_v7_calib_60.csv',\n",
    "    'submission_v7_calib_62.csv',\n",
    "    'submission_v7_calib_65.csv',\n",
    "    'submission_v7_final_blend.csv',\n",
    "    'submission_v7_final_rank.csv',\n",
    "]\n",
    "\n",
    "print(\"\\nSubmissions created:\")\n",
    "for i, fname in enumerate(v7_files, 1):\n",
    "    if os.path.exists(fname):\n",
    "        print(f\"  {i}. {fname}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOP PICKS TO TRY:\")\n",
    "print(\"  1. submission_v7_multiseed_5.csv - reduces variance\")\n",
    "print(\"  2. submission_v7_stack_rank.csv - diverse model blend\")\n",
    "print(\"  3. submission_v7_final_blend.csv - combines best of all\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
