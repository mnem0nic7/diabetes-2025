{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7a2d9bd",
   "metadata": {},
   "source": [
    "# Diabetes Prediction Model V2 - Advanced\n",
    "## Kaggle Playground Series S5E12\n",
    "\n",
    "Improvements:\n",
    "1. Optuna hyperparameter tuning\n",
    "2. Advanced feature engineering  \n",
    "3. Target encoding\n",
    "4. Stacking ensemble with meta-learner\n",
    "5. Optimized ensemble weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfef689a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gallison/workspace/kaggle/diabetes_prediction_challenge/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, TargetEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "print(\"Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee9fe41",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29d6a56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (700000, 26), Test: (300000, 25)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "print(f\"Train: {train.shape}, Test: {test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65e82f7",
   "metadata": {},
   "source": [
    "## 2. Advanced Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ead736c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape after FE: (700000, 57)\n",
      "Test shape after FE: (300000, 56)\n",
      "New features created: 31\n"
     ]
    }
   ],
   "source": [
    "def advanced_feature_engineering(df):\n",
    "    \"\"\"Advanced feature engineering with more interactions and domain knowledge\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # === BASIC FEATURES (from v1) ===\n",
    "    # BMI categories\n",
    "    df['bmi_category'] = pd.cut(df['bmi'], bins=[0, 18.5, 25, 30, 100], \n",
    "                                 labels=['underweight', 'normal', 'overweight', 'obese'])\n",
    "    \n",
    "    # Age groups\n",
    "    df['age_group'] = pd.cut(df['age'], bins=[0, 30, 45, 60, 100], \n",
    "                              labels=['young', 'middle', 'senior', 'elderly'])\n",
    "    \n",
    "    # Blood pressure categories\n",
    "    df['bp_category'] = pd.cut(df['systolic_bp'], bins=[0, 120, 130, 140, 200], \n",
    "                                labels=['normal', 'elevated', 'high_stage1', 'high_stage2'])\n",
    "    \n",
    "    # Cholesterol ratios\n",
    "    df['ldl_hdl_ratio'] = df['ldl_cholesterol'] / (df['hdl_cholesterol'] + 1)\n",
    "    df['total_hdl_ratio'] = df['cholesterol_total'] / (df['hdl_cholesterol'] + 1)\n",
    "    df['non_hdl_cholesterol'] = df['cholesterol_total'] - df['hdl_cholesterol']\n",
    "    \n",
    "    # Risk score\n",
    "    df['medical_risk_score'] = (df['family_history_diabetes'] + \n",
    "                                 df['hypertension_history'] + \n",
    "                                 df['cardiovascular_history'])\n",
    "    \n",
    "    # Activity and lifestyle\n",
    "    df['activity_score'] = df['physical_activity_minutes_per_week'] / (df['screen_time_hours_per_day'] + 1)\n",
    "    df['lifestyle_score'] = (df['diet_score'] + df['sleep_hours_per_day'] + \n",
    "                              df['physical_activity_minutes_per_week'] / 60 - \n",
    "                              df['screen_time_hours_per_day'] - df['alcohol_consumption_per_week'])\n",
    "    \n",
    "    # Blood pressure\n",
    "    df['pulse_pressure'] = df['systolic_bp'] - df['diastolic_bp']\n",
    "    df['mean_arterial_pressure'] = df['diastolic_bp'] + (df['pulse_pressure'] / 3)\n",
    "    \n",
    "    # Interactions\n",
    "    df['age_bmi_interaction'] = df['age'] * df['bmi']\n",
    "    df['physical_activity_per_day'] = df['physical_activity_minutes_per_week'] / 7\n",
    "    \n",
    "    # === NEW ADVANCED FEATURES ===\n",
    "    \n",
    "    # More interaction features\n",
    "    df['age_systolic_interaction'] = df['age'] * df['systolic_bp']\n",
    "    df['bmi_triglycerides'] = df['bmi'] * df['triglycerides']\n",
    "    df['age_cholesterol'] = df['age'] * df['cholesterol_total']\n",
    "    df['activity_bmi_ratio'] = df['physical_activity_minutes_per_week'] / (df['bmi'] + 1)\n",
    "    \n",
    "    # Family history interactions (most important feature!)\n",
    "    df['family_age_risk'] = df['family_history_diabetes'] * df['age']\n",
    "    df['family_bmi_risk'] = df['family_history_diabetes'] * df['bmi']\n",
    "    df['family_activity_protection'] = df['family_history_diabetes'] * df['physical_activity_minutes_per_week']\n",
    "    \n",
    "    # Metabolic syndrome indicators\n",
    "    df['metabolic_risk'] = ((df['bmi'] > 30).astype(int) + \n",
    "                            (df['triglycerides'] > 150).astype(int) + \n",
    "                            (df['hdl_cholesterol'] < 40).astype(int) +\n",
    "                            (df['systolic_bp'] > 130).astype(int))\n",
    "    \n",
    "    # Triglyceride to HDL ratio (insulin resistance marker)\n",
    "    df['tg_hdl_ratio'] = df['triglycerides'] / (df['hdl_cholesterol'] + 1)\n",
    "    \n",
    "    # Cardiovascular risk score\n",
    "    df['cv_risk_score'] = (df['systolic_bp'] / 10 + df['ldl_cholesterol'] / 10 + \n",
    "                           df['age'] / 5 - df['hdl_cholesterol'] / 10)\n",
    "    \n",
    "    # Lifestyle quality score\n",
    "    df['healthy_lifestyle'] = ((df['physical_activity_minutes_per_week'] > 150).astype(int) +\n",
    "                                (df['diet_score'] > 7).astype(int) +\n",
    "                                (df['sleep_hours_per_day'] >= 7).astype(int) +\n",
    "                                (df['alcohol_consumption_per_week'] < 7).astype(int) +\n",
    "                                (df['screen_time_hours_per_day'] < 4).astype(int))\n",
    "    \n",
    "    # Age-related risk categories\n",
    "    df['age_risk'] = np.where(df['age'] < 40, 0, \n",
    "                     np.where(df['age'] < 50, 1,\n",
    "                     np.where(df['age'] < 60, 2, 3)))\n",
    "    \n",
    "    # BMI squared (non-linear relationship)\n",
    "    df['bmi_squared'] = df['bmi'] ** 2\n",
    "    \n",
    "    # Log transformations for skewed features\n",
    "    df['log_triglycerides'] = np.log1p(df['triglycerides'])\n",
    "    df['log_physical_activity'] = np.log1p(df['physical_activity_minutes_per_week'])\n",
    "    \n",
    "    # Waist-hip and BMI combined risk\n",
    "    df['obesity_indicator'] = df['bmi'] * df['waist_to_hip_ratio']\n",
    "    \n",
    "    # Sleep quality indicator\n",
    "    df['poor_sleep'] = ((df['sleep_hours_per_day'] < 6) | (df['sleep_hours_per_day'] > 9)).astype(int)\n",
    "    \n",
    "    # Sedentary lifestyle\n",
    "    df['sedentary'] = ((df['physical_activity_minutes_per_week'] < 75) & \n",
    "                        (df['screen_time_hours_per_day'] > 6)).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "train_fe = advanced_feature_engineering(train)\n",
    "test_fe = advanced_feature_engineering(test)\n",
    "\n",
    "print(f\"Train shape after FE: {train_fe.shape}\")\n",
    "print(f\"Test shape after FE: {test_fe.shape}\")\n",
    "print(f\"New features created: {train_fe.shape[1] - train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c1bbb5",
   "metadata": {},
   "source": [
    "## 3. Target Encoding for Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d011d272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label encoded 6 categorical columns\n",
      "Features after target encoding: 55\n",
      "Target encoded columns: ['bmi_category_target_enc', 'age_group_target_enc', 'bp_category_target_enc']\n",
      "\n",
      "Feature dtypes:\n",
      "int64      31\n",
      "float64    24\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Prepare features\n",
    "target = 'diagnosed_diabetes'\n",
    "drop_cols = ['id', target, 'bmi_category', 'age_group', 'bp_category']\n",
    "cat_cols = ['bmi_category', 'age_group', 'bp_category']\n",
    "\n",
    "# Identify original string categorical columns that need encoding\n",
    "original_cat_cols = ['gender', 'ethnicity', 'education_level', 'income_level', 'smoking_status', 'employment_status']\n",
    "\n",
    "# Separate features\n",
    "X = train_fe.drop(columns=[c for c in drop_cols if c in train_fe.columns])\n",
    "y = train_fe[target]\n",
    "X_test = test_fe.drop(columns=[c for c in drop_cols if c in test_fe.columns])\n",
    "\n",
    "# Label encode the original categorical columns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "for col in original_cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    # Fit on all unique values from both train and test\n",
    "    all_values = pd.concat([X[col], X_test[col]], axis=0).unique()\n",
    "    le.fit(all_values)\n",
    "    X[col] = le.transform(X[col])\n",
    "    X_test[col] = le.transform(X_test[col])\n",
    "\n",
    "print(f\"Label encoded {len(original_cat_cols)} categorical columns\")\n",
    "\n",
    "# Target encoding for engineered categorical columns\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "\n",
    "# Get categorical columns that exist in X\n",
    "existing_cat_cols = [c for c in cat_cols if c in train_fe.columns]\n",
    "\n",
    "# Fit target encoder on full training data\n",
    "target_encoder = TargetEncoder(target_type='binary', smooth='auto')\n",
    "\n",
    "# Convert categorical columns back to original data for encoding\n",
    "train_cats = train_fe[existing_cat_cols].copy()\n",
    "test_cats = test_fe[existing_cat_cols].copy()\n",
    "\n",
    "# Fit on training data\n",
    "target_encoder.fit(train_cats, y)\n",
    "\n",
    "# Transform both train and test\n",
    "train_encoded = target_encoder.transform(train_cats)\n",
    "test_encoded = target_encoder.transform(test_cats)\n",
    "\n",
    "# Add encoded features to X\n",
    "for i, col in enumerate(existing_cat_cols):\n",
    "    X[f'{col}_target_enc'] = train_encoded[:, i]\n",
    "    X_test[f'{col}_target_enc'] = test_encoded[:, i]\n",
    "\n",
    "print(f\"Features after target encoding: {X.shape[1]}\")\n",
    "print(f\"Target encoded columns: {[f'{c}_target_enc' for c in existing_cat_cols]}\")\n",
    "print(f\"\\nFeature dtypes:\\n{X.dtypes.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1028d779",
   "metadata": {},
   "source": [
    "## 4. Optuna Hyperparameter Tuning for LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f653420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Optuna optimization for LightGBM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.725647: 100%|██████████| 50/50 [45:12<00:00, 54.25s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best LightGBM CV AUC: 0.72565\n",
      "Best parameters: {'learning_rate': 0.16650679360420056, 'n_estimators': 606, 'max_depth': 7, 'num_leaves': 20, 'min_child_samples': 94, 'subsample': 0.7718478966632039, 'colsample_bytree': 0.5805949017577372, 'reg_alpha': 1.4322521289019877e-07, 'reg_lambda': 1.1952348229966282e-07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna_integration import LightGBMPruningCallback\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "def objective_lgb(trial):\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbosity': -1,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 42,\n",
    "        \n",
    "        # Hyperparameters to tune\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "    }\n",
    "    \n",
    "    cv_scores = []\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(50, verbose=False),\n",
    "                LightGBMPruningCallback(trial, 'auc')\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        preds = model.predict_proba(X_val)[:, 1]\n",
    "        score = roc_auc_score(y_val, preds)\n",
    "        cv_scores.append(score)\n",
    "    \n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "# Run Optuna optimization (50 trials for speed, increase for better results)\n",
    "print(\"Starting Optuna optimization for LightGBM...\")\n",
    "study_lgb = optuna.create_study(direction='maximize', study_name='lgb_diabetes')\n",
    "study_lgb.optimize(objective_lgb, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest LightGBM CV AUC: {study_lgb.best_value:.5f}\")\n",
    "print(f\"Best parameters: {study_lgb.best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850ec116",
   "metadata": {},
   "source": [
    "## 5. Train Optimized Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a176dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters configured based on Optuna optimization\n"
     ]
    }
   ],
   "source": [
    "# Get best LightGBM params from Optuna\n",
    "best_lgb_params = study_lgb.best_params.copy()\n",
    "best_lgb_params.update({\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 42\n",
    "})\n",
    "\n",
    "# XGBoost params (similar structure)\n",
    "xgb_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'n_estimators': best_lgb_params.get('n_estimators', 500),\n",
    "    'learning_rate': best_lgb_params.get('learning_rate', 0.05),\n",
    "    'max_depth': best_lgb_params.get('max_depth', 6),\n",
    "    'subsample': best_lgb_params.get('subsample', 0.8),\n",
    "    'colsample_bytree': best_lgb_params.get('colsample_bytree', 0.8),\n",
    "    'reg_alpha': best_lgb_params.get('reg_alpha', 0.1),\n",
    "    'reg_lambda': best_lgb_params.get('reg_lambda', 0.1),\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "# CatBoost params\n",
    "cat_params = {\n",
    "    'iterations': best_lgb_params.get('n_estimators', 500),\n",
    "    'learning_rate': best_lgb_params.get('learning_rate', 0.05),\n",
    "    'depth': min(best_lgb_params.get('max_depth', 6), 10),\n",
    "    'l2_leaf_reg': best_lgb_params.get('reg_lambda', 0.1),\n",
    "    'random_seed': 42,\n",
    "    'verbose': False,\n",
    "    'eval_metric': 'AUC',\n",
    "    'early_stopping_rounds': 50\n",
    "}\n",
    "\n",
    "print(\"Model parameters configured based on Optuna optimization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "629dce6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models with 5-fold CV...\n",
      "\n",
      "=== Fold 1 ===\n",
      "  LightGBM: 0.72634\n",
      "  XGBoost:  0.71528\n",
      "  CatBoost: 0.72329\n",
      "\n",
      "=== Fold 2 ===\n",
      "  LightGBM: 0.72471\n",
      "  XGBoost:  0.71456\n",
      "  CatBoost: 0.72074\n",
      "\n",
      "=== Fold 3 ===\n",
      "  LightGBM: 0.72474\n",
      "  XGBoost:  0.71497\n",
      "  CatBoost: 0.72205\n",
      "\n",
      "=== Fold 4 ===\n",
      "  LightGBM: 0.72598\n",
      "  XGBoost:  0.71681\n",
      "  CatBoost: 0.72320\n",
      "\n",
      "=== Fold 5 ===\n",
      "  LightGBM: 0.72645\n",
      "  XGBoost:  0.71558\n",
      "  CatBoost: 0.72279\n",
      "\n",
      "==================================================\n",
      "LightGBM CV AUC: 0.72565 (+/- 0.00076)\n",
      "XGBoost CV AUC:  0.71544 (+/- 0.00076)\n",
      "CatBoost CV AUC: 0.72241 (+/- 0.00094)\n"
     ]
    }
   ],
   "source": [
    "# Train all models with CV and collect OOF predictions for stacking\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Storage for OOF predictions (for stacking) and test predictions\n",
    "oof_lgb = np.zeros(len(X))\n",
    "oof_xgb = np.zeros(len(X))\n",
    "oof_cat = np.zeros(len(X))\n",
    "\n",
    "test_preds_lgb = np.zeros(len(X_test))\n",
    "test_preds_xgb = np.zeros(len(X_test))\n",
    "test_preds_cat = np.zeros(len(X_test))\n",
    "\n",
    "cv_scores_lgb = []\n",
    "cv_scores_xgb = []\n",
    "cv_scores_cat = []\n",
    "\n",
    "print(\"Training models with 5-fold CV...\\n\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    print(f\"=== Fold {fold + 1} ===\")\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # LightGBM\n",
    "    lgb_model = lgb.LGBMClassifier(**best_lgb_params)\n",
    "    lgb_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        callbacks=[lgb.early_stopping(50, verbose=False)]\n",
    "    )\n",
    "    oof_lgb[val_idx] = lgb_model.predict_proba(X_val)[:, 1]\n",
    "    test_preds_lgb += lgb_model.predict_proba(X_test)[:, 1] / n_splits\n",
    "    score_lgb = roc_auc_score(y_val, oof_lgb[val_idx])\n",
    "    cv_scores_lgb.append(score_lgb)\n",
    "    print(f\"  LightGBM: {score_lgb:.5f}\")\n",
    "    \n",
    "    # XGBoost\n",
    "    xgb_model = xgb.XGBClassifier(**xgb_params)\n",
    "    xgb_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "    oof_xgb[val_idx] = xgb_model.predict_proba(X_val)[:, 1]\n",
    "    test_preds_xgb += xgb_model.predict_proba(X_test)[:, 1] / n_splits\n",
    "    score_xgb = roc_auc_score(y_val, oof_xgb[val_idx])\n",
    "    cv_scores_xgb.append(score_xgb)\n",
    "    print(f\"  XGBoost:  {score_xgb:.5f}\")\n",
    "    \n",
    "    # CatBoost\n",
    "    cat_model = CatBoostClassifier(**cat_params)\n",
    "    cat_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=(X_val, y_val),\n",
    "        verbose=False\n",
    "    )\n",
    "    oof_cat[val_idx] = cat_model.predict_proba(X_val)[:, 1]\n",
    "    test_preds_cat += cat_model.predict_proba(X_test)[:, 1] / n_splits\n",
    "    score_cat = roc_auc_score(y_val, oof_cat[val_idx])\n",
    "    cv_scores_cat.append(score_cat)\n",
    "    print(f\"  CatBoost: {score_cat:.5f}\")\n",
    "    print()\n",
    "\n",
    "# Overall CV scores\n",
    "print(\"=\" * 50)\n",
    "print(f\"LightGBM CV AUC: {np.mean(cv_scores_lgb):.5f} (+/- {np.std(cv_scores_lgb):.5f})\")\n",
    "print(f\"XGBoost CV AUC:  {np.mean(cv_scores_xgb):.5f} (+/- {np.std(cv_scores_xgb):.5f})\")\n",
    "print(f\"CatBoost CV AUC: {np.mean(cv_scores_cat):.5f} (+/- {np.std(cv_scores_cat):.5f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5c11ba",
   "metadata": {},
   "source": [
    "## 6. Stacking Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16deb5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Meta-Model CV AUC: 0.72624 (+/- 0.00074)\n",
      "\n",
      "Meta-model coefficients: LGB=3.369, XGB=0.678, CAT=0.915\n"
     ]
    }
   ],
   "source": [
    "# Create stacking features from OOF predictions\n",
    "stack_train = np.column_stack([oof_lgb, oof_xgb, oof_cat])\n",
    "stack_test = np.column_stack([test_preds_lgb, test_preds_xgb, test_preds_cat])\n",
    "\n",
    "# Train meta-model (Logistic Regression) with CV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "meta_model = LogisticRegression(C=1.0, random_state=42, max_iter=1000)\n",
    "meta_scores = []\n",
    "\n",
    "# Cross-validate the meta-model\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(stack_train, y)):\n",
    "    X_meta_train, X_meta_val = stack_train[train_idx], stack_train[val_idx]\n",
    "    y_meta_train, y_meta_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    meta_model.fit(X_meta_train, y_meta_train)\n",
    "    meta_preds = meta_model.predict_proba(X_meta_val)[:, 1]\n",
    "    meta_scores.append(roc_auc_score(y_meta_val, meta_preds))\n",
    "\n",
    "print(f\"Stacking Meta-Model CV AUC: {np.mean(meta_scores):.5f} (+/- {np.std(meta_scores):.5f})\")\n",
    "\n",
    "# Final meta-model training on all data\n",
    "meta_model.fit(stack_train, y)\n",
    "stack_preds = meta_model.predict_proba(stack_test)[:, 1]\n",
    "\n",
    "print(f\"\\nMeta-model coefficients: LGB={meta_model.coef_[0][0]:.3f}, XGB={meta_model.coef_[0][1]:.3f}, CAT={meta_model.coef_[0][2]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ac5a0b",
   "metadata": {},
   "source": [
    "## 7. Optimize Ensemble Weights with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12b18040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing ensemble weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 53. Best value: 0.726248: 100%|██████████| 100/100 [00:17<00:00,  5.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal weights: LGB=0.716, XGB=0.150, CAT=0.134\n",
      "Weighted Ensemble CV AUC: 0.72625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Find optimal weights for simple weighted average ensemble\n",
    "def objective_weights(trial):\n",
    "    w_lgb = trial.suggest_float('w_lgb', 0, 1)\n",
    "    w_xgb = trial.suggest_float('w_xgb', 0, 1)\n",
    "    w_cat = trial.suggest_float('w_cat', 0, 1)\n",
    "    \n",
    "    # Normalize weights\n",
    "    total = w_lgb + w_xgb + w_cat\n",
    "    w_lgb, w_xgb, w_cat = w_lgb/total, w_xgb/total, w_cat/total\n",
    "    \n",
    "    # Weighted ensemble using OOF predictions\n",
    "    ensemble_oof = w_lgb * oof_lgb + w_xgb * oof_xgb + w_cat * oof_cat\n",
    "    return roc_auc_score(y, ensemble_oof)\n",
    "\n",
    "print(\"Optimizing ensemble weights...\")\n",
    "study_weights = optuna.create_study(direction='maximize')\n",
    "study_weights.optimize(objective_weights, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "# Get optimal weights\n",
    "best_weights = study_weights.best_params\n",
    "total = best_weights['w_lgb'] + best_weights['w_xgb'] + best_weights['w_cat']\n",
    "w_lgb = best_weights['w_lgb'] / total\n",
    "w_xgb = best_weights['w_xgb'] / total\n",
    "w_cat = best_weights['w_cat'] / total\n",
    "\n",
    "print(f\"\\nOptimal weights: LGB={w_lgb:.3f}, XGB={w_xgb:.3f}, CAT={w_cat:.3f}\")\n",
    "print(f\"Weighted Ensemble CV AUC: {study_weights.best_value:.5f}\")\n",
    "\n",
    "# Create weighted ensemble predictions\n",
    "weighted_ensemble = w_lgb * test_preds_lgb + w_xgb * test_preds_xgb + w_cat * test_preds_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f111292e",
   "metadata": {},
   "source": [
    "## 8. Final Predictions & Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c108a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final OOF CV AUC Scores:\n",
      "  Simple Average:    0.72539\n",
      "  Weighted Average:  0.72625\n",
      "  Stacking:          0.72622\n",
      "\n",
      "Best method: weighted with CV AUC: 0.72625\n"
     ]
    }
   ],
   "source": [
    "# Compare all ensemble methods\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "simple_avg = (test_preds_lgb + test_preds_xgb + test_preds_cat) / 3\n",
    "\n",
    "# Validate on OOF predictions\n",
    "simple_avg_oof = (oof_lgb + oof_xgb + oof_cat) / 3\n",
    "weighted_oof = w_lgb * oof_lgb + w_xgb * oof_xgb + w_cat * oof_cat\n",
    "stack_oof = cross_val_predict(\n",
    "    LogisticRegression(C=1.0, random_state=42, max_iter=1000),\n",
    "    stack_train, y, cv=5, method='predict_proba'\n",
    ")[:, 1]\n",
    "\n",
    "print(\"Final OOF CV AUC Scores:\")\n",
    "print(f\"  Simple Average:    {roc_auc_score(y, simple_avg_oof):.5f}\")\n",
    "print(f\"  Weighted Average:  {roc_auc_score(y, weighted_oof):.5f}\")\n",
    "print(f\"  Stacking:          {roc_auc_score(y, stack_oof):.5f}\")\n",
    "\n",
    "# Choose the best ensemble\n",
    "best_method = max([\n",
    "    ('simple_avg', roc_auc_score(y, simple_avg_oof), simple_avg),\n",
    "    ('weighted', roc_auc_score(y, weighted_oof), weighted_ensemble),\n",
    "    ('stacking', roc_auc_score(y, stack_oof), stack_preds)\n",
    "], key=lambda x: x[1])\n",
    "\n",
    "print(f\"\\nBest method: {best_method[0]} with CV AUC: {best_method[1]:.5f}\")\n",
    "final_preds = best_method[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c5a14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to submission_v2.csv\n",
      "Shape: (300000, 2)\n",
      "\n",
      "Prediction statistics:\n",
      "count    300000.000000\n",
      "mean          0.601488\n",
      "std           0.192705\n",
      "min           0.042057\n",
      "25%           0.463048\n",
      "50%           0.606288\n",
      "75%           0.747484\n",
      "max           0.994777\n",
      "Name: diagnosed_diabetes, dtype: float64\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'diagnosed_diabetes': final_preds\n",
    "})\n",
    "submission.to_csv('submission_v2.csv', index=False)\n",
    "\n",
    "print(f\"Submission saved to submission_v2.csv\")\n",
    "print(f\"Shape: {submission.shape}\")\n",
    "print(f\"\\nPrediction statistics:\")\n",
    "print(submission['diagnosed_diabetes'].describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
